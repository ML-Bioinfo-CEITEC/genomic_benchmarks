{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_datasets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMk84eMN7zK2Hd514eKYJJt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovb0gcusfZoB"
      },
      "source": [
        "## Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18uHqMhfXWyu"
      },
      "source": [
        "!mkdir ../../datasets/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3B68s8GkzYn",
        "outputId": "ad752720-18ec-42e2-bf43-98f4d4c150f5"
      },
      "source": [
        "!git clone https://github.com/katarinagresova/ensembl_scraper.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ensembl_scraper'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 185 (delta 116), reused 119 (delta 56), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (185/185), 47.62 KiB | 9.52 MiB/s, done.\n",
            "Resolving deltas: 100% (116/116), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eqi2c_zqBJg",
        "outputId": "4ebdbdce-a230-40d1-fa1f-0e141d667aa2"
      },
      "source": [
        "%cd ensembl_scraper/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ensembl_scraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI-R7RsBmyDD",
        "outputId": "598abcd7-02d0-4ac5-c8f8-af4826f0dc94"
      },
      "source": [
        "!python -m pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bio==0.7.7\n",
            "  Downloading bio-0.7.7-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting biopython==1.79\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 16.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer==2.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.0.4)\n",
            "Collecting idna==3.2\n",
            "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.0.1)\n",
            "Collecting numpy==1.21.2\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 194 kB/s \n",
            "\u001b[?25hCollecting pandas==1.3.2\n",
            "  Downloading pandas-1.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 27.6 MB/s \n",
            "\u001b[?25hCollecting plac==1.3.3\n",
            "  Downloading plac-1.3.3-py2.py3-none-any.whl (22 kB)\n",
            "Collecting pyfiglet==0.8.post1\n",
            "  Downloading pyfiglet-0.8.post1-py2.py3-none-any.whl (865 kB)\n",
            "\u001b[K     |████████████████████████████████| 865 kB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (2.8.2)\n",
            "Collecting pytz==2021.1\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 57.2 MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting requests==2.26.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 674 kB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 30 kB/s \n",
            "\u001b[?25hCollecting scipy==1.7.1\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 50 kB/s \n",
            "\u001b[?25hCollecting six==1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting threadpoolctl==2.2.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Collecting tqdm==4.62.2\n",
            "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting twobitreader==3.1.7\n",
            "  Downloading twobitreader-3.1.7.tar.gz (9.2 kB)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 42.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: twobitreader\n",
            "  Building wheel for twobitreader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twobitreader: filename=twobitreader-3.1.7-py3-none-any.whl size=9604 sha256=d944caabbdd976511d66dec0d539dbd96c5e394b480eeabda6beae09bbd216b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/4f/70/282382e10dc13fd946f157888d9652437b01e3ac4541ac4b4b\n",
            "Successfully built twobitreader\n",
            "Installing collected packages: urllib3, six, numpy, idna, tqdm, threadpoolctl, scipy, requests, pytz, plac, biopython, twobitreader, scikit-learn, PyYAML, pyfiglet, pandas, bio\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: plac\n",
            "    Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 7.4.0 requires plac<1.2.0,>=0.9.6, but you have plac 1.3.3 which is incompatible.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
            "tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "spacy 2.2.4 requires plac<1.2.0,>=0.9.6, but you have plac 1.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-5.4.1 bio-0.7.7 biopython-1.79 idna-3.2 numpy-1.21.2 pandas-1.3.2 plac-1.3.3 pyfiglet-0.8.post1 pytz-2021.1 requests-2.26.0 scikit-learn-0.24.2 scipy-1.7.1 six-1.16.0 threadpoolctl-2.2.0 tqdm-4.62.2 twobitreader-3.1.7 urllib3-1.26.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v22_DrTgYDOh"
      },
      "source": [
        "## Create config file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nEuX47IX8Yj"
      },
      "source": [
        "import yaml\n",
        "\n",
        "config = {\n",
        "    \"root_dir\": \"../../datasets/\",\n",
        "    \"organisms\": {\n",
        "        \"mus_musculus\": {\n",
        "            \"external_feature\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "user_config = 'user_config.yaml'\n",
        "with open(user_config, 'w') as handle:\n",
        "  yaml.dump(config, handle)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVFl4NtwffLK"
      },
      "source": [
        "## Run tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsjaflkOSUVS",
        "outputId": "9a5df293-02a7-4f80-8b0c-2dbf6736d2a4"
      },
      "source": [
        "!python scraper/ensembl_scraper.py -c user_config.yaml"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rProcessing organisms:   0% 0/1 [00:00<?, ?it/s]\n",
            "Processing feature files:   0% 0/1 [00:00<?, ?it/s]\u001b[AINFO:root:download_file(): Going to download file from path ftp://ftp.ensembl.org/pub/release-100/mysql/regulation_mart_100/mmusculus_external_feature__external_feature__main.txt.gz\n",
            "INFO:root:download_file(): File downloaded to path ../../datasets//tmp//mus_musculus_external_feature.txt.gz.\n",
            "INFO:root:parse_feature_file(): Going to parse file ../../datasets//tmp//mus_musculus_external_feature.txt.gz\n",
            "INFO:root:parse_feature_file(): Done parsing file ../../datasets//tmp//mus_musculus_external_feature.txt.gz\n",
            "\n",
            "\n",
            "Processing feature types:   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[AINFO:root:find_sequences_and_save_to_fasta(): Going to find sequences based on genomic loci.\n",
            "INFO:root:download_2bit_file(): Going to download 2bit file mm10\n",
            "INFO:root:download_2bit_file(): File for mm10 downloaded to path ../../datasets//tmp/mm10.2bit.\n",
            "INFO:root:find_sequences_and_save_to_fasta(): Done finding sequences.\n",
            "INFO:root:remove_low_quality(): Going to preprocess sequences.\n",
            "INFO:root:remove_low_quality(): Original number of sequences: 619\n",
            "INFO:root:remove_low_quality(): Number of sequences after outlier rejection: 605\n",
            "INFO:root:remove_low_quality(): Number of sequences after Ns rejection: 605\n",
            "INFO:root:remove_low_quality(): Done preprocessing sequences.\n",
            "INFO:root:download_2bit_file(): Going to download 2bit file mm10\n",
            "INFO:root:download_2bit_file(): File for mm10 already exists. Not going to download.\n",
            "INFO:root:download_2bit_file(): Going to download 2bit file mm10\n",
            "INFO:root:download_2bit_file(): File for mm10 already exists. Not going to download.\n",
            "\n",
            "\n",
            "Processing feature types: 100% 1/1 [00:10<00:00, 10.21s/it]\n",
            "\n",
            "Processing feature files: 100% 1/1 [00:11<00:00, 11.65s/it]\n",
            "Processing organisms: 100% 1/1 [00:11<00:00, 11.65s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhgkIgONh0bt"
      },
      "source": [
        "## Results\n",
        "\n",
        "You can find created dataset at `/datasets` path in Google Colab. Or some other path you specified as `root_dir` in config file."
      ]
    }
  ]
}