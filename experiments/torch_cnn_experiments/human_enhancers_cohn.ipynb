{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn\n",
    "from genomic_benchmarks.models.torch_cnn import CNN\n",
    "from utils import coll_factory, LetterTokenizer, build_vocab, check_seq_lengths, check_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config is correct\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"use_padding\": False,\n",
    "    \"run_on_gpu\": True,\n",
    "    \"dataset\": HumanEnhancersCohn,\n",
    "    \"number_of_classes\": 1,\n",
    "    \"dataset_version\": 0,\n",
    "    \"force_download\": False,\n",
    "    \"epochs\": 15,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "check_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /home/jupyter/.genomic_benchmarks/translated_human_enhancers_cohn/human_enhancers_cohn.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "get_dataset_fn = config[\"dataset\"]\n",
    "train_dset = get_dataset_fn('train', force_download=config[\"force_download\"], version=config[\"dataset_version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 7\n",
      "{'<eos>': 5, 'G': 4, 'A': 3, 'C': 2, '<pad>': 6, 'T': 1, '<bos>': 0}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(LetterTokenizer())\n",
    "vocabulary = build_vocab(train_dset, tokenizer, use_padding=config[\"use_padding\"])\n",
    "\n",
    "print(\"vocab len:\" ,vocabulary.__len__())\n",
    "print(vocabulary.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader and batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "max_seq_len  500\n"
     ]
    }
   ],
   "source": [
    "# Run on GPU or CPU\n",
    "device = 'cuda' if config[\"run_on_gpu\"] and torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "max_seq_len, nn_input_len = check_seq_lengths(dataset=train_dset, config=config)\n",
    "\n",
    "# Data Loader\n",
    "if(config[\"use_padding\"]):\n",
    "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = nn_input_len)\n",
    "else:\n",
    "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    number_of_classes=config[\"number_of_classes\"],\n",
    "    vocab_size=vocabulary.__len__(),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    input_len=nn_input_len\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train metrics: \n",
      " Accuracy: 65.9%, Avg loss: 0.653561 \n",
      "\n",
      "Epoch 1\n",
      "Train metrics: \n",
      " Accuracy: 61.6%, Avg loss: 0.654188 \n",
      "\n",
      "Epoch 2\n",
      "Train metrics: \n",
      " Accuracy: 67.7%, Avg loss: 0.649225 \n",
      "\n",
      "Epoch 3\n",
      "Train metrics: \n",
      " Accuracy: 69.5%, Avg loss: 0.641505 \n",
      "\n",
      "Epoch 4\n",
      "Train metrics: \n",
      " Accuracy: 68.5%, Avg loss: 0.636271 \n",
      "\n",
      "Epoch 5\n",
      "Train metrics: \n",
      " Accuracy: 70.7%, Avg loss: 0.630740 \n",
      "\n",
      "Epoch 6\n",
      "Train metrics: \n",
      " Accuracy: 70.4%, Avg loss: 0.631523 \n",
      "\n",
      "Epoch 7\n",
      "Train metrics: \n",
      " Accuracy: 68.9%, Avg loss: 0.631073 \n",
      "\n",
      "Epoch 8\n",
      "Train metrics: \n",
      " Accuracy: 71.0%, Avg loss: 0.633175 \n",
      "\n",
      "Epoch 9\n",
      "Train metrics: \n",
      " Accuracy: 72.1%, Avg loss: 0.624318 \n",
      "\n",
      "Epoch 10\n",
      "Train metrics: \n",
      " Accuracy: 72.5%, Avg loss: 0.625214 \n",
      "\n",
      "Epoch 11\n",
      "Train metrics: \n",
      " Accuracy: 72.7%, Avg loss: 0.630044 \n",
      "\n",
      "Epoch 12\n",
      "Train metrics: \n",
      " Accuracy: 72.7%, Avg loss: 0.622582 \n",
      "\n",
      "Epoch 13\n",
      "Train metrics: \n",
      " Accuracy: 73.4%, Avg loss: 0.619494 \n",
      "\n",
      "Epoch 14\n",
      "Train metrics: \n",
      " Accuracy: 71.8%, Avg loss: 0.619672 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, epochs=config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /home/jupyter/.genomic_benchmarks/translated_human_enhancers_cohn/human_enhancers_cohn.zip... Done.\n",
      "Unzipping...Done.\n",
      "test_loss  140.0959752202034\n",
      "num_batches 218\n",
      "correct 4662\n",
      "size 6948\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.642642 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dset = get_dataset_fn('test', force_download=config[\"force_download\"], version=config[\"dataset_version\"])\n",
    "test_loader = DataLoader(test_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "\n",
    "model.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "9828b828580f1cac1b571b33de6cff8bacecc8916095e1bcbc967952ca7105b7"
  },
  "kernelspec": {
   "display_name": "Python [conda env:bench_env]",
   "language": "python",
   "name": "conda-env-bench_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
