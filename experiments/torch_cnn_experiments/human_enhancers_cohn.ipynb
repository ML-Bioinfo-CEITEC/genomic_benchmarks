{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn\n",
    "from genomic_benchmarks.models.torch_cnn import CNN\n",
    "from genomic_benchmarks.dataset_getters.utils import coll_factory, LetterTokenizer, build_vocab, check_seq_lengths, check_config \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config is correct\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"use_padding\": False,\n",
    "    \"run_on_gpu\": True,\n",
    "    \"dataset\": HumanEnhancersCohn,\n",
    "    \"number_of_classes\": 2,\n",
    "    \"dataset_version\": 0,\n",
    "    \"force_download\": False,\n",
    "    \"epochs\": 15,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"batch_size\": 32,\n",
    "#   vocabulary that is not present in the training set but is present in the test set\n",
    "    \"vocab_to_add\": [],\n",
    "}\n",
    "check_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /home/martinekvlastimil95/.genomic_benchmarks/translated_human_enhancers_cohn/human_enhancers_cohn.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "get_dataset_fn = config[\"dataset\"]\n",
    "train_dset = get_dataset_fn('train', force_download=config[\"force_download\"], version=config[\"dataset_version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 6\n",
      "{'<eos>': 5, 'G': 4, 'A': 3, 'C': 2, 'T': 1, '<bos>': 0}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(LetterTokenizer())\n",
    "vocabulary = build_vocab(train_dset, tokenizer, use_padding=config[\"use_padding\"])\n",
    "\n",
    "print(\"vocab len:\" ,vocabulary.__len__())\n",
    "print(vocabulary.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader and batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "max_seq_len  500\n"
     ]
    }
   ],
   "source": [
    "# Run on GPU or CPU\n",
    "device = 'cuda' if config[\"run_on_gpu\"] and torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "max_seq_len, nn_input_len = check_seq_lengths(dataset=train_dset, config=config)\n",
    "\n",
    "# Data Loader\n",
    "if(config[\"use_padding\"]):\n",
    "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = nn_input_len)\n",
    "else:\n",
    "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    number_of_classes=config[\"number_of_classes\"],\n",
    "    vocab_size=vocabulary.__len__(),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    input_len=nn_input_len\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train metrics: \n",
      " Accuracy: 63.8%, Avg loss: 0.654706 \n",
      "\n",
      "Epoch 1\n",
      "Train metrics: \n",
      " Accuracy: 63.5%, Avg loss: 0.654620 \n",
      "\n",
      "Epoch 2\n",
      "Train metrics: \n",
      " Accuracy: 65.6%, Avg loss: 0.648896 \n",
      "\n",
      "Epoch 3\n",
      "Train metrics: \n",
      " Accuracy: 65.1%, Avg loss: 0.648712 \n",
      "\n",
      "Epoch 4\n",
      "Train metrics: \n",
      " Accuracy: 67.2%, Avg loss: 0.639706 \n",
      "\n",
      "Epoch 5\n",
      "Train metrics: \n",
      " Accuracy: 68.3%, Avg loss: 0.636274 \n",
      "\n",
      "Epoch 6\n",
      "Train metrics: \n",
      " Accuracy: 68.7%, Avg loss: 0.633181 \n",
      "\n",
      "Epoch 7\n",
      "Train metrics: \n",
      " Accuracy: 71.3%, Avg loss: 0.630310 \n",
      "\n",
      "Epoch 8\n",
      "Train metrics: \n",
      " Accuracy: 69.9%, Avg loss: 0.630840 \n",
      "\n",
      "Epoch 9\n",
      "Train metrics: \n",
      " Accuracy: 69.0%, Avg loss: 0.628594 \n",
      "\n",
      "Epoch 10\n",
      "Train metrics: \n",
      " Accuracy: 71.4%, Avg loss: 0.626259 \n",
      "\n",
      "Epoch 11\n",
      "Train metrics: \n",
      " Accuracy: 69.7%, Avg loss: 0.626749 \n",
      "\n",
      "Epoch 12\n",
      "Train metrics: \n",
      " Accuracy: 71.3%, Avg loss: 0.623227 \n",
      "\n",
      "Epoch 13\n",
      "Train metrics: \n",
      " Accuracy: 71.9%, Avg loss: 0.621211 \n",
      "\n",
      "Epoch 14\n",
      "Train metrics: \n",
      " Accuracy: 72.9%, Avg loss: 0.619314 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, epochs=config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /home/martinekvlastimil95/.genomic_benchmarks/translated_human_enhancers_cohn/human_enhancers_cohn.zip... Done.\n",
      "Unzipping...Done.\n",
      "test_loss  140.57966035604477\n",
      "num_batches 218\n",
      "correct 4723\n",
      "size 6948\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.644861 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dset = get_dataset_fn('test', force_download=config[\"force_download\"], version=config[\"dataset_version\"])\n",
    "test_loader = DataLoader(test_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "\n",
    "model.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "c0ce2704b40e8f5ab266e985c6dfdf52dff618cf568b21da2fc16697057ae39e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
