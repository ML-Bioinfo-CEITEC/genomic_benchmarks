{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH CNN Classifier\n",
    "\n",
    "To run this notebook on an another benchmark, use\n",
    "\n",
    "```\n",
    "papermill utils/torch_cnn_classifier.ipynb torch_cnn_experiments/[DATASET NAME].ipynb -p DATASET [DATASET NAME]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# DATASET = 'no_dataset'\n",
    "DATASET = 'demo_human_or_worm'\n",
    "VERSION = 0\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "ITER = 999\n",
    "PATIENCE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_human_or_worm 0 32 10 999 1\n"
     ]
    }
   ],
   "source": [
    "print(DATASET, VERSION, BATCH_SIZE, EPOCHS, ITER, PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be imported first\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import get_dataset\n",
    "from genomic_benchmarks.models.torch import CNN\n",
    "from genomic_benchmarks.dataset_getters.utils import coll_factory, LetterTokenizer, build_vocab, check_seq_lengths, check_config, VARIABLE_LENGTH_DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_human_or_worm_999_\n"
     ]
    }
   ],
   "source": [
    "USE_PADDING = DATASET in VARIABLE_LENGTH_DATASETS\n",
    "    \n",
    "config = {\n",
    "    \"dataset\": DATASET,\n",
    "    \"dataset_version\": VERSION,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"use_padding\": USE_PADDING,\n",
    "    \"force_download\": False,\n",
    "    \"run_on_gpu\": True,\n",
    "    \"number_of_classes\": 2,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"patience\": PATIENCE\n",
    "}\n",
    "checkpoint_name = config[\"dataset\"] + \"_\" + str(ITER) + \"_\" \n",
    "print(checkpoint_name)\n",
    "\n",
    "# check_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/davidcechak/genomic-cnn/fbb010b6be0b4c4585b3cca225a3ddca\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(project_name=\"genomic-cnn\", api_key=\"EpKIINrla6U4B4LJhd9Sv4i0b\")\n",
    "experiment.log_parameters(config)\n",
    "experiment.set_name(checkpoint_name + \"patience:\" + str(config[\"patience\"]) + \"_\" + \"epochs:\" + str(config[\"epochs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "train_dset = get_dataset(config[\"dataset\"], 'train')\n",
    "t_size = int(len(train_dset)*0.8)\n",
    "v_size = len(train_dset)-t_size\n",
    "\n",
    "new_train_dset, valid_dset = data.random_split(train_dset, [t_size, v_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 9\n",
      "{'<pad>': 8, 'T': 5, 'A': 4, 'C': 3, '<eos>': 6, 'G': 2, '<bos>': 1, 'N': 7, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(LetterTokenizer())\n",
    "vocabulary = build_vocab(train_dset, tokenizer, use_padding=config[\"use_padding\"])\n",
    "\n",
    "print(\"vocab len:\" ,vocabulary.__len__())\n",
    "print(vocabulary.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader and batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "max_seq_len  200\n",
      "not all sequences are of the same length\n"
     ]
    }
   ],
   "source": [
    "# Run on GPU or CPU\n",
    "device = 'cuda' if config[\"run_on_gpu\"] and torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "max_seq_len, nn_input_len = check_seq_lengths(dataset=train_dset, config=config)\n",
    "\n",
    "# Data Loader\n",
    "if(config[\"use_padding\"]):\n",
    "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = nn_input_len)\n",
    "else:\n",
    "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
    "\n",
    "train_loader = DataLoader(new_train_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    number_of_classes=config[\"number_of_classes\"],\n",
    "    vocab_size=vocabulary.__len__(),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    input_len=nn_input_len\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7976931348623157e+308\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcechak/genomic_benchmarks/src/genomic_benchmarks/dataset_getters/utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(pad(x), dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics: \n",
      " Accuracy: 91.9%, Avg loss: 0.545157 \n",
      "\n",
      "Train metrics: \n",
      " Accuracy: 91.9%, Avg loss: 0.541906 \n",
      "\n",
      "new best valid loss in epoch 0 -> saving new checkpoint\n",
      "Epoch 1\n",
      "Valid metrics: \n",
      " Accuracy: 90.3%, Avg loss: 0.547571 \n",
      "\n",
      "Train metrics: \n",
      " Accuracy: 90.1%, Avg loss: 0.544864 \n",
      "\n",
      "not improved for epochs: 1\n",
      "ending training\n",
      "loading best model from epoch 0\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_loader, \n",
    "    valid_loader, \n",
    "    epochs=config[\"epochs\"], \n",
    "    patience = config[\"patience\"], \n",
    "    checkpoint_name = checkpoint_name, \n",
    "    experiment = experiment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p  12500 ; tp  11415.738284111023 ; fp  947.0352419841711\n",
      "recall  0.9132590627288818 ; precision  0.923396215259854\n",
      "num_batches 782\n",
      "correct 22997\n",
      "size 25000\n",
      "Test metrics: \n",
      " Accuracy: 0.919880, F1 score: 0.918300, Avg loss: 0.542281 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.91988, 0.918299663722506)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dset = get_dataset(config[\"dataset\"], 'test')\n",
    "test_loader = DataLoader(test_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "\n",
    "acc, f1 = model.test(test_loader, experiment = experiment)\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "conda-env-bench_env-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
