{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PYTORCH CNN Classifier\n",
                "\n",
                "To run this notebook on an another benchmark, use\n",
                "\n",
                "```\n",
                "papermill utils/torch_cnn_classifier.ipynb torch_cnn_experiments/[DATASET NAME].ipynb -p DATASET [DATASET NAME]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# DATASET = 'no_dataset'\n",
                "DATASET = 'demo_human_or_worm'\n",
                "VERSION = 0\n",
                "BATCH_SIZE = 64\n",
                "EPOCHS = 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "demo_human_or_worm 0 64 1\n"
                    ]
                }
            ],
            "source": [
                "print(DATASET, VERSION, BATCH_SIZE, EPOCHS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import DataLoader\n",
                "from torchtext.data.utils import get_tokenizer\n",
                "\n",
                "from genomic_benchmarks.data_check import is_downloaded, info\n",
                "from genomic_benchmarks.dataset_getters.pytorch_datasets import get_dataset\n",
                "from genomic_benchmarks.loc2seq import download_dataset\n",
                "from genomic_benchmarks.models.torch import CNN\n",
                "from genomic_benchmarks.dataset_getters.utils import coll_factory, LetterTokenizer, build_vocab, check_seq_lengths, check_config, VARIABLE_LENGTH_DATASETS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "USE_PADDING = DATASET in VARIABLE_LENGTH_DATASETS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Choose the dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not is_downloaded(DATASET):\n",
                "    download_dataset(DATASET, local_repo=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset `demo_human_or_worm` has 2 classes: human, worm.\n",
                        "\n",
                        "All lengths of genomic intervals equals 200.\n",
                        "\n",
                        "Totally 100000 sequences have been found, 75000 for training and 25000 for testing.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/katarina/git/genomic_benchmarks/src/genomic_benchmarks/utils/datasets.py:50: UserWarning: No version specified. Using version 0.\n",
                        "  warnings.warn(f\"No version specified. Using version {metadata['version']}.\")\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>train</th>\n",
                            "      <th>test</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>human</th>\n",
                            "      <td>37500</td>\n",
                            "      <td>12500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>worm</th>\n",
                            "      <td>37500</td>\n",
                            "      <td>12500</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "       train   test\n",
                            "human  37500  12500\n",
                            "worm   37500  12500"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "info(DATASET, local_repo=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dset = get_dataset(DATASET, 'train')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "2"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "NUM_CLASSES = len(set([train_dset[i][1] for i in range(len(train_dset))]))\n",
                "NUM_CLASSES"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenizer and vocab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "vocab len: 8\n",
                        "{'C': 5, 'A': 4, '<eos>': 6, 'G': 3, 'T': 2, '<bos>': 1, 'N': 7, '<unk>': 0}\n"
                    ]
                }
            ],
            "source": [
                "tokenizer = get_tokenizer(LetterTokenizer())\n",
                "vocabulary = build_vocab(train_dset, tokenizer, use_padding=USE_PADDING)\n",
                "\n",
                "print(\"vocab len:\" ,vocabulary.__len__())\n",
                "print(vocabulary.get_stoi())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataloader and batch preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using cpu device\n",
                        "max_seq_len  200\n"
                    ]
                }
            ],
            "source": [
                "# Run on GPU or CPU\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print('Using {} device'.format(device))\n",
                "\n",
                "max_seq_len, nn_input_len = check_seq_lengths(dataset=train_dset, use_padding=USE_PADDING)\n",
                "\n",
                "# Data Loader\n",
                "if(USE_PADDING):\n",
                "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = nn_input_len)\n",
                "else:\n",
                "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
                "\n",
                "train_loader = DataLoader(train_dset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = CNN(\n",
                "    number_of_classes=NUM_CLASSES,\n",
                "    vocab_size=vocabulary.__len__(),\n",
                "    embedding_dim=100,\n",
                "    input_len=nn_input_len,\n",
                "    device=device\n",
                ").to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 0\n",
                        "Train metrics: \n",
                        " Accuracy: 88.9%, Avg loss: 0.553672 \n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "model.fit(train_loader, epochs=EPOCHS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test metrics: \n",
                        " Accuracy: 0.903280, F1 score: 0.909064, Avg loss: 0.558611 \n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(0.90328, 0.9090635577284693)"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_dset = get_dataset(DATASET, 'test')\n",
                "test_loader = DataLoader(test_dset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
                "\n",
                "acc, f1 = model.test(test_loader)\n",
                "acc, f1"
            ]
        }
    ],
    "metadata": {
        "environment": {
            "name": "pytorch-gpu.1-9.m75",
            "type": "gcloud",
            "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
        },
        "interpreter": {
            "hash": "5fb5174addf958ec7b3e9e5d35a565dfd5bab1ae69383cd521f52756e68c7fc3"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('venv': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
