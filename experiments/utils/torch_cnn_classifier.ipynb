{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PYTORCH CNN Classifier\n",
                "\n",
                "To run this notebook on an another benchmark, use\n",
                "\n",
                "```\n",
                "papermill utils/torch_cnn_classifier.ipynb torch_cnn_experiments/[DATASET NAME].ipynb -p DATASET [DATASET NAME]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# DATASET = 'no_dataset'\n",
                "DATASET = 'demo_human_or_worm'\n",
                "VERSION = 0\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "demo_human_or_worm 0 32 1\n"
                    ]
                }
            ],
            "source": [
                "print(DATASET, VERSION, BATCH_SIZE, EPOCHS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['human_nontata_promoters',\n",
                            " 'human_enhancers_ensembl',\n",
                            " 'dummy_mouse_enhancers_ensembl',\n",
                            " 'demo_human_or_worm',\n",
                            " 'human_enhancers_cohn',\n",
                            " 'demo_coding_vs_intergenomic_seqs']"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import DataLoader\n",
                "from torchtext.data.utils import get_tokenizer\n",
                "\n",
                "from genomic_benchmarks.dataset_getters.pytorch_datasets import get_dataset\n",
                "from genomic_benchmarks.models.torch import CNN\n",
                "from genomic_benchmarks.dataset_getters.utils import coll_factory, LetterTokenizer, build_vocab, check_seq_lengths, check_config, VARIABLE_LENGTH_DATASETS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "USE_PADDING = DATASET in VARIABLE_LENGTH_DATASETS\n",
                "    \n",
                "config = {\n",
                "    \"dataset\": DATASET,\n",
                "    \"dataset_version\": VERSION,\n",
                "    \"epochs\": EPOCHS,\n",
                "    \"batch_size\": BATCH_SIZE,\n",
                "    \"use_padding\": USE_PADDING,\n",
                "    \"force_download\": False,\n",
                "    \"run_on_gpu\": True,\n",
                "    \"number_of_classes\": 2,\n",
                "    \"embedding_dim\": 100,\n",
                "}\n",
                "check_config(config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Choose the dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dset = get_dataset(config[\"dataset\"], 'train')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenizer and vocab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "vocab len: 9\n",
                        "{'<eos>': 6, 'G': 5, 'C': 4, 'A': 3, '<pad>': 8, 'T': 2, '<bos>': 1, 'N': 7, '<unk>': 0}\n"
                    ]
                }
            ],
            "source": [
                "tokenizer = get_tokenizer(LetterTokenizer())\n",
                "vocabulary = build_vocab(train_dset, tokenizer, use_padding=config[\"use_padding\"])\n",
                "\n",
                "print(\"vocab len:\" ,vocabulary.__len__())\n",
                "print(vocabulary.get_stoi())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataloader and batch preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using cuda device\n",
                        "max_seq_len  200\n",
                        "not all sequences are of the same length\n"
                    ]
                }
            ],
            "source": [
                "# Run on GPU or CPU\n",
                "device = 'cuda' if config[\"run_on_gpu\"] and torch.cuda.is_available() else 'cpu'\n",
                "print('Using {} device'.format(device))\n",
                "\n",
                "max_seq_len, nn_input_len = check_seq_lengths(dataset=train_dset, config=config)\n",
                "\n",
                "# Data Loader\n",
                "if(config[\"use_padding\"]):\n",
                "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = nn_input_len)\n",
                "else:\n",
                "    collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
                "\n",
                "train_loader = DataLoader(train_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
                        "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
                    ]
                }
            ],
            "source": [
                "model = CNN(\n",
                "    number_of_classes=config[\"number_of_classes\"],\n",
                "    vocab_size=vocabulary.__len__(),\n",
                "    embedding_dim=config[\"embedding_dim\"],\n",
                "    input_len=nn_input_len\n",
                ").to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 0\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/jupyter/genomic_benchmarks/src/genomic_benchmarks/dataset_getters/utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
                        "  x = torch.tensor(pad(x), dtype=torch.long)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train metrics: \n",
                        " Accuracy: 91.1%, Avg loss: 0.543994 \n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "model.train(train_loader, epochs=config[\"epochs\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "p  12500 ; tp  11078.16893029213 ; fp  794.1601008734349\n",
                        "recall  0.8862535144233704 ; precision  0.9331083144016041\n",
                        "num_batches 782\n",
                        "correct 22801\n",
                        "size 25000\n",
                        "Test metrics: \n",
                        " Accuracy: 91.2%, F1 score: 0.909078, Avg loss: 0.543700 \n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(0.9090775786036839, 91.204)"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_dset = get_dataset(config[\"dataset\"], 'test')\n",
                "test_loader = DataLoader(test_dset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
                "\n",
                "acc, f1 = model.test(test_loader)\n",
                "acc, f1"
            ]
        }
    ],
    "metadata": {
        "environment": {
            "name": "pytorch-gpu.1-9.m75",
            "type": "gcloud",
            "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
        },
        "interpreter": {
            "hash": "9828b828580f1cac1b571b33de6cff8bacecc8916095e1bcbc967952ca7105b7"
        },
        "kernelspec": {
            "display_name": "conda-env-bench_env-py",
            "language": "python",
            "name": "bench_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}