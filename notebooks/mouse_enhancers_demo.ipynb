{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda list --export > requirements_conda.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoMouseEnhancers\n",
    "from utils import simple_coll, padding_coll_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic CNN model\n",
    "class NeuralNetwork(nn.Module):\n",
    "        \n",
    "    def __init__(self, number_of_classes, vocab_size, embedding_dim, context_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=16, kernel_size=8, bias=True)\n",
    "        self.norm1 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=8, bias=True)\n",
    "        self.norm2 = nn.BatchNorm1d(8)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=8, out_channels=4, kernel_size=8, bias=True)\n",
    "        self.norm3 = nn.BatchNorm1d(4)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=4, out_channels=3, kernel_size=8, bias=True)\n",
    "        self.norm4 = nn.BatchNorm1d(3)\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(861, 512)\n",
    "        self.lin2 = nn.Linear(512, number_of_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.lin1(x) \n",
    "        x = self.lin2(x) \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "    def train_loop(self, dataloader, optimizer):\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = self(x)\n",
    "#             print('pred: ', pred)\n",
    "#             print('y: ', y)\n",
    "            loss = self.loss(pred, y)\n",
    "#             print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "#       train acc\n",
    "# todo: optimize counting of acc\n",
    "        size = dataloader.dataset.__len__()\n",
    "        num_batches = len(dataloader)\n",
    "        train_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = self(X)\n",
    "                train_loss += self.loss(pred, y).item()\n",
    "                correct += (torch.round(pred) == y).sum().item()\n",
    "\n",
    "#         print('train_loss ', train_loss)\n",
    "#         print('num_batches', num_batches)\n",
    "#         print('correct', correct)\n",
    "#         print('size', size)\n",
    "        train_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Train metrics: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "            \n",
    "            \n",
    "    def train(self, dataloader, epochs):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t}\")\n",
    "            self.train_loop(dataloader, optimizer)\n",
    "\n",
    "    def test(self, dataloader):\n",
    "        size = dataloader.dataset.__len__()\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = self(X)\n",
    "                test_loss += self.loss(pred, y).item()\n",
    "                correct += (torch.round(pred) == y).sum().item()\n",
    "\n",
    "        print('test_loss ', test_loss)\n",
    "        print('num_batches', num_batches)\n",
    "        print('correct', correct)\n",
    "        print('size', size)\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference /home/martinekvlastimil95/.genomic_benchmarks/fasta/Mus_musculus.GRCm38.dna_rm.toplevel.fa.gz already exists. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinekvlastimil95/repos/genomic_benchmarks/src/genomic_benchmarks/loc2seq/with_biopython.py:87: UserWarning: No version specified. Using version 0.\n",
      "  warnings.warn(f\"No version specified. Using version {metadata['version']}.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7795b6244a1841c39e69a09ad99cd1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('TTCCCCAGTTCACCTCAAAGAGGAATGGGGAAGGCAAATTGCTCGGGGCAGTTCCCAAGCGCAGCACCAATGGAGCTCGGTGGAGCTCTCCAAAGCTGGGCCTGTGAGAAGACTCCCTGGCTTGGATTCTGGCCTCTTTAAGGCCCAGCGCTATGGCTTCCACAGATCTTGTTTAAACTTGAGAGGGTGGGGGAGGGGCCAAGGGAAACTACATTCTTCCCCACCGAAGCCAAGAGTCTAAAGTAACACTTCTGGTTGTGAGTGATGGCAGTCTGTTTGTGATTATGGCCCTGGCTTTCTGGCAGGGCAGCCCTGGAACCCCACAGTCTTGCTGACGGGAGGGACACTCTCCCTCACTTGGGTAATCTTGTCCTCATCTGACATGACCAAAGACATTCATCAGCCCCTCTGCTTCCAACAGGTCTCCATGGCAACTCCTCAAAAACACTATTTAAAGAGGGGGGGAAAGCCACCCCAAAACCCTATCGTGAAATTCTTTCTGTTCCAGGCACGTCCTAAAGAACCCAAGAAGCATCAGCATGGGCTGATGTCATCTGCAGGAGGGAGGGGACGCCGGACCATCCTCTCCCTCTTCCTTTCATCAGGGAGAACAATGCATGCACACATGGATGCCCCTTTGCTGCCACAGTCTGAGGCGGCTCGGGGAAGCTCAGCTGTGCTCTCTCCCGGAAATTTAATCTGCGAAGCATTTGGCCTCGAAAACTCAATTCCAAAGGTGTCCGGGAGCCCACCAACCCTGTGATCATTGTTACAAATCCAGGGCTGCAAGAGTCCGACCCGTTTGGCAGCTGGGGAGAGCTGGCTGGGGGATGGGCAACATGGCCACTGTCCCCTCCCCTGTGCCTCCTGTGTGGAGAGAGACTGTATGTTTATACAGGCCCAGCGGCGGTGGGGAGGTAGCATAACTCTCTAAGATCTCCACACCGTGGGACAGAGAGGCTACTTCTGGCTCGGTTGATAAAAGTGCAGTGGTTAGGGAATAGGAGAGCGTCAGATCCAAGAAGCAACTGTGGGCA',\n",
       " 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose the dataset\n",
    "get_dataset_fn = DemoMouseEnhancers\n",
    "train_dset = get_dataset_fn('train', force_download=False)\n",
    "train_dset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('N', 664278), ('T', 436517), ('A', 436144), ('C', 364382), ('G', 360709), ('<bos>', 968), ('<eos>', 968)]\n",
      "vocab len: 8\n",
      "{'N': 6, '<eos>': 5, 'G': 4, 'A': 3, 'C': 2, '<pad>': 7, 'T': 1, '<bos>': 0}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab, build_vocab_from_iterator\n",
    "from collections import Counter \n",
    "\n",
    "class LetterTokenizer():\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "    def __call__(self, items):\n",
    "        if isinstance(items, str):\n",
    "            return self.__tokenize_str(items)\n",
    "        else:\n",
    "            return (self.__tokenize_str(t) for t in items)\n",
    "    def __tokenize_str(self, t):\n",
    "        tokenized = list(t.replace(\"\\n\",\"\"))\n",
    "        tokenized.append('<eos>')\n",
    "        tokenized.insert(0,'<bos>')\n",
    "        return tokenized\n",
    "\n",
    "tokenizer = get_tokenizer(LetterTokenizer())\n",
    "\n",
    "def build_vocab(dataset, tokenizer):\n",
    "    counter = Counter()\n",
    "    for i in range(len(dataset)):\n",
    "        counter.update(tokenizer(dataset[i][0]))\n",
    "    print(counter.most_common())\n",
    "    builded_voc = vocab(counter)\n",
    "    builded_voc.append_token('<pad>')\n",
    "    return builded_voc\n",
    "\n",
    "# todo: why build fn does not work as expected (iterator argument)\n",
    "#     return build_vocab_from_iterator(\n",
    "#         iterator = counter, \n",
    "#         specials = ['<unk>', '<pad>', '<bos>', '<eos>'],\n",
    "#         special_first = True)\n",
    "\n",
    "vocabulary = build_vocab(train_dset, tokenizer)\n",
    "print(\"vocab len:\" ,vocabulary.__len__())\n",
    "print(vocabulary.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(train_dset[0][0])\n",
    "# train_dset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use collate with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_len  4707\n"
     ]
    }
   ],
   "source": [
    "# use collate with padding\n",
    "input_len = max([len(train_dset[i][0]) for i in range(len(train_dset))])\n",
    "print(\"input_len \", input_len)\n",
    "# padding_coll_factory(longest_length, vocab, tokenizer):\n",
    "collate = padding_coll_factory(input_len, vocabulary, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinekvlastimil95/repos/genomic_benchmarks/notebooks/utils.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(tmp, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5720/3293701803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(next(iter(train_loader))[0][0])\n",
    "print(next(iter(train_loader))[1][0])\n",
    "print(next(iter(train_loader))[0][2])\n",
    "print(next(iter(train_loader))[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: \n",
      " Accuracy: 68.8%, Avg loss: 0.623178 \n",
      "\n",
      "Epoch 1\n",
      "Train metrics: \n",
      " Accuracy: 77.4%, Avg loss: 0.613179 \n",
      "\n",
      "Epoch 2\n",
      "Train metrics: \n",
      " Accuracy: 75.6%, Avg loss: 0.601661 \n",
      "\n",
      "Epoch 3\n",
      "Train metrics: \n",
      " Accuracy: 78.1%, Avg loss: 0.597339 \n",
      "\n",
      "Epoch 4\n",
      "Train metrics: \n",
      " Accuracy: 76.0%, Avg loss: 0.598450 \n",
      "\n",
      "Epoch 5\n",
      "Train metrics: \n",
      " Accuracy: 80.9%, Avg loss: 0.580879 \n",
      "\n",
      "Epoch 6\n",
      "Train metrics: \n",
      " Accuracy: 81.0%, Avg loss: 0.583884 \n",
      "\n",
      "Epoch 7\n",
      "Train metrics: \n",
      " Accuracy: 79.9%, Avg loss: 0.583817 \n",
      "\n",
      "Epoch 8\n",
      "Train metrics: \n",
      " Accuracy: 78.0%, Avg loss: 0.588464 \n",
      "\n",
      "Epoch 9\n",
      "Train metrics: \n",
      " Accuracy: 81.4%, Avg loss: 0.577699 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# __init__(self, number_of_classes, vocab_size, embedding_dim, context_size):\n",
    "model = NeuralNetwork(\n",
    "    number_of_classes=1,\n",
    "    vocab_size=vocabulary.__len__(),\n",
    "    embedding_dim=100,\n",
    "    context_size=input_len\n",
    ").cuda()\n",
    "model.train(train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference /home/martinekvlastimil95/.genomic_benchmarks/fasta/Mus_musculus.GRCm38.dna_rm.toplevel.fa.gz already exists. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinekvlastimil95/repos/genomic_benchmarks/src/genomic_benchmarks/loc2seq/with_biopython.py:87: UserWarning: No version specified. Using version 0.\n",
      "  warnings.warn(f\"No version specified. Using version {metadata['version']}.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999b7bec7cca44f881fac250f4e28a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  4.785072445869446\n",
      "num_batches 8\n",
      "correct 190\n",
      "size 242\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.598134 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dset = get_dataset_fn('test', force_download=False)\n",
    "test_loader = DataLoader(test_dset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "model.test(test_loader)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "2223b3d6c1f07b7eb0c5454f24b1253839434c1cd4ca3fc7d48d892ce989c008"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('bench_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
