{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanNontataPromoters\n",
    "from utils import coll_factory, LetterTokenizer, build_vocab\n",
    "from cnn_model import CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference /home/jupyter/.genomic_benchmarks/fasta/Homo_sapiens.GRCh38.dna.toplevel.fa.gz already exists. Skipping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1d330c06ca4fa1a84d9f68bb8ff632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_dataset_fn = DemoHumanNontataPromoters\n",
    "train_dset = get_dataset_fn('train', force_download=False, version=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 7\n",
      "{'N': 6, 'T': 3, '<eos>': 5, 'G': 2, 'C': 4, 'A': 1, '<bos>': 0}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(LetterTokenizer())\n",
    "vocabulary = build_vocab(train_dset, tokenizer, use_padding=False)\n",
    "vocabulary.append_token('N')\n",
    "\n",
    "\n",
    "print(\"vocab len:\" ,vocabulary.__len__())\n",
    "print(vocabulary.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch preparation with collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "RUN_ON_GPU = True\n",
    "device = 'cuda' if RUN_ON_GPU and torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "collate = coll_factory(vocabulary, tokenizer, device)\n",
    "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_len  251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bench_env/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = max([len(train_dset[i][0]) for i in range(len(train_dset))])\n",
    "print(\"max_seq_len \", max_seq_len)\n",
    "# Count in added in tokenizer '<bos>' and '<eos>' \n",
    "nn_input_len = max_seq_len+2\n",
    "\n",
    "model = CNN(\n",
    "    number_of_classes=1,\n",
    "    vocab_size=vocabulary.__len__(),\n",
    "    embedding_dim=100,\n",
    "    input_len=nn_input_len\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train metrics: \n",
      " Accuracy: 80.0%, Avg loss: 0.616821 \n",
      "\n",
      "Epoch 1\n",
      "Train metrics: \n",
      " Accuracy: 82.9%, Avg loss: 0.606362 \n",
      "\n",
      "Epoch 2\n",
      "Train metrics: \n",
      " Accuracy: 83.4%, Avg loss: 0.603001 \n",
      "\n",
      "Epoch 3\n",
      "Train metrics: \n",
      " Accuracy: 83.0%, Avg loss: 0.599323 \n",
      "\n",
      "Epoch 4\n",
      "Train metrics: \n",
      " Accuracy: 84.2%, Avg loss: 0.595568 \n",
      "\n",
      "Epoch 5\n",
      "Train metrics: \n",
      " Accuracy: 81.1%, Avg loss: 0.600004 \n",
      "\n",
      "Epoch 6\n",
      "Train metrics: \n",
      " Accuracy: 85.0%, Avg loss: 0.593839 \n",
      "\n",
      "Epoch 7\n",
      "Train metrics: \n",
      " Accuracy: 85.1%, Avg loss: 0.593579 \n",
      "\n",
      "Epoch 8\n",
      "Train metrics: \n",
      " Accuracy: 85.7%, Avg loss: 0.590632 \n",
      "\n",
      "Epoch 9\n",
      "Train metrics: \n",
      " Accuracy: 85.9%, Avg loss: 0.588893 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference /home/jupyter/.genomic_benchmarks/fasta/Homo_sapiens.GRCh38.dna.toplevel.fa.gz already exists. Skipping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c1a24407a94818811e0e5071ccd27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  168.8089716732502\n",
      "num_batches 283\n",
      "correct 7632\n",
      "size 9034\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.596498 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dset = get_dataset_fn('test', force_download=False, version=0)\n",
    "test_loader = DataLoader(test_dset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "model.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "9828b828580f1cac1b571b33de6cff8bacecc8916095e1bcbc967952ca7105b7"
  },
  "kernelspec": {
   "display_name": "Python [conda env:bench_env]",
   "language": "python",
   "name": "conda-env-bench_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
