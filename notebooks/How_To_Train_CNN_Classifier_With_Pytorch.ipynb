{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "pytorch-gpu.1-9.m75",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
    },
    "interpreter": {
      "hash": "c0ce2704b40e8f5ab266e985c6dfdf52dff618cf568b21da2fc16697057ae39e"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "name": "How_To_Train_CNN_Classifier_With_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-Bioinfo-CEITEC/genomic_benchmarks/blob/main/notebooks/How_To_Train_CNN_Classifier_With_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyJNsvSFac3K"
      },
      "source": [
        "# How To Train CNN Classifier With Pytorch\n",
        "\n",
        "This notebook demonstrates how to use `genomic_benchmarks` to train a neural network classifier on one of its benchmark datasets [human_nontata_promoters](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/tree/main/docs/human_nontata_promoters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuhcuLhWapla",
        "outputId": "4a98454d-13dc-48f9-bc14-a8bd03572ff8"
      },
      "source": [
        "pip install genomic-benchmarks"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks.git\n",
            "  Cloning https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks.git to /tmp/pip-req-build-01c7_jsw\n",
            "  Running command git clone -q https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks.git /tmp/pip-req-build-01c7_jsw\n",
            "Collecting biopython>=1.79\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from genomic-benchmarks==0.0.3) (2.23.0)\n",
            "Requirement already satisfied: pip>=20.0.1 in /usr/local/lib/python3.7/dist-packages (from genomic-benchmarks==0.0.3) (21.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from genomic-benchmarks==0.0.3) (1.19.5)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from genomic-benchmarks==0.0.3) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from genomic-benchmarks==0.0.3) (4.62.3)\n",
            "Collecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader>=0.4 in /usr/local/lib/python3.7/dist-packages (from genomic-benchmarks==0.0.3) (0.4)\n",
            "Collecting yarl\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->genomic-benchmarks==0.0.3) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->genomic-benchmarks==0.0.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->genomic-benchmarks==0.0.3) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->genomic-benchmarks==0.0.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->genomic-benchmarks==0.0.3) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->genomic-benchmarks==0.0.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->genomic-benchmarks==0.0.3) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl->genomic-benchmarks==0.0.3) (3.10.0.2)\n",
            "Collecting multidict>=4.0\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 57.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: genomic-benchmarks\n",
            "  Building wheel for genomic-benchmarks (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for genomic-benchmarks: filename=genomic_benchmarks-0.0.3-py3-none-any.whl size=25944 sha256=015625b8ddc54906066209466571b5b31e948aab13d4704fb1b3c73ee3c22448\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rshfz4kv/wheels/b2/22/e4/91b7bd669628972e32032ea3c61a4bf5d0d33bb752fe8674d5\n",
            "Successfully built genomic-benchmarks\n",
            "Installing collected packages: multidict, yarl, pyyaml, biopython, genomic-benchmarks\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed biopython-1.79 genomic-benchmarks-0.0.3 multidict-5.2.0 pyyaml-6.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmKgWPNvac3W"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn\n",
        "from genomic_benchmarks.models.torch import CNN\n",
        "from genomic_benchmarks.dataset_getters.utils import coll_factory, LetterTokenizer, build_vocab\n",
        "from genomic_benchmarks.data_check import info\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ekph6Vqac3X"
      },
      "source": [
        "# Choose the dataset\n",
        "\n",
        "Create pytorch dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O1p1rMbac3X",
        "outputId": "7decf898-8284-44b0-a817-dba5c97ce940"
      },
      "source": [
        "train_dset =  HumanEnhancersCohn('train', version=0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /root/.genomic_benchmarks/human_enhancers_cohn.zip... Done.\n",
            "Unzipping...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JACj9LGQac3Y"
      },
      "source": [
        "# Print out information about the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "Vkg8YbBtac3Z",
        "outputId": "be834dc4-c4a3-4755-cdaf-7c1f13d6ea58"
      },
      "source": [
        "info(\"human_enhancers_cohn\", 0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset `human_enhancers_cohn` has 2 classes: negative, positive.\n",
            "\n",
            "All lenghts of genomic intervals equals 500.\n",
            "\n",
            "Totally 27791 sequences have been found, 20843 for training and 6948 for testing.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>10422</td>\n",
              "      <td>3474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>10421</td>\n",
              "      <td>3474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          train  test\n",
              "negative  10422  3474\n",
              "positive  10421  3474"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO9Gs2tFac3a"
      },
      "source": [
        "# Tokenizer and vocab\n",
        "\n",
        "Create tokenizer for the dataset, so we can numericalize the data and feed them to neural network. \n",
        "From the dataset info we can notice that all sequences are of the same length, hence we will use no padding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRY9_1NRac3b",
        "outputId": "444eb109-2951-4320-f6b7-6329141759de"
      },
      "source": [
        "tokenizer = get_tokenizer(LetterTokenizer())\n",
        "vocabulary = build_vocab(train_dset, tokenizer, use_padding=False)\n",
        "\n",
        "print(\"vocab len:\" ,vocabulary.__len__())\n",
        "print(vocabulary.get_stoi())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab len: 7\n",
            "{'<eos>': 6, '<bos>': 1, '<unk>': 0, 'C': 5, 'A': 2, 'T': 3, 'G': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8_bIg2ac3c"
      },
      "source": [
        "# Dataloader and batch preparation\n",
        "\n",
        "We will create pytorch data loader, which will preprocess and batch the data for the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVjAMp9Qac3d",
        "outputId": "18adf004-7381-4beb-91a6-c8c8ba9f1750"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))\n",
        "\n",
        "collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
        "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True, collate_fn=collate)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfVIba63ac3d"
      },
      "source": [
        "# Model\n",
        "We will initialize our model.\n",
        "From the dataset info, we know that all inputs are 500 characters long, and the number of classes is 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXHLGdMsac3e"
      },
      "source": [
        "model = CNN(\n",
        "    number_of_classes=2,\n",
        "    vocab_size=vocabulary.__len__(),\n",
        "    embedding_dim=100,\n",
        "    input_len=500\n",
        ").to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXSfvH6bac3e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FThlj7Y6ac3e",
        "outputId": "578ef82c-a861-4d68-ffe3-262f4c75b37e"
      },
      "source": [
        "model.train(train_loader, epochs=5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Train metrics: \n",
            " Accuracy: 65.4%, Avg loss: 0.645070 \n",
            "\n",
            "Epoch 1\n",
            "Train metrics: \n",
            " Accuracy: 67.7%, Avg loss: 0.637024 \n",
            "\n",
            "Epoch 2\n",
            "Train metrics: \n",
            " Accuracy: 68.1%, Avg loss: 0.637149 \n",
            "\n",
            "Epoch 3\n",
            "Train metrics: \n",
            " Accuracy: 67.3%, Avg loss: 0.636471 \n",
            "\n",
            "Epoch 4\n",
            "Train metrics: \n",
            " Accuracy: 70.7%, Avg loss: 0.630674 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzADCTJac3f"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akWdmV6nac3f"
      },
      "source": [
        "test_dset = HumanEnhancersCohn('test', version=0)\n",
        "test_loader = DataLoader(test_dset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "predictions = []\n",
        "for x,y in test_loader:\n",
        "  output = torch.round(model(x))\n",
        "  for prediction in output.tolist():\n",
        "    predictions.append(prediction[0])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyROI4K_NYtq",
        "outputId": "97b8f282-b1d3-4794-e99c-c027fde9d9a2"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from genomic_benchmarks.data_check.info import labels_in_order\n",
        "\n",
        "labels = labels_in_order(dset_name='human_enhancers_cohn')\n",
        "f1_score(labels, predictions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40943812595484635"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}