{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-Bioinfo-CEITEC/genomic_benchmarks/blob/main/notebooks/How_To_Train_CNN_Classifier_With_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyJNsvSFac3K"
      },
      "source": [
        "# How To Train CNN Classifier With Pytorch\n",
        "\n",
        "This notebook demonstrates how to use `genomic_benchmarks` to train a neural network classifier on one of its benchmark datasets [human_nontata_promoters](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/tree/main/docs/human_nontata_promoters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuhcuLhWapla",
        "outputId": "4a98454d-13dc-48f9-bc14-a8bd03572ff8"
      },
      "outputs": [],
      "source": [
        "#If you work in Google Colaboratory - uncomment the following line to install the package to your virtual machine  \n",
        "#!pip install -qq tensorflow_addons genomic-benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mmKgWPNvac3W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn\n",
        "from genomic_benchmarks.models.torch import CNN\n",
        "from genomic_benchmarks.dataset_getters.utils import coll_factory, LetterTokenizer, build_vocab\n",
        "from genomic_benchmarks.data_check import info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ekph6Vqac3X"
      },
      "source": [
        "# Choose the dataset\n",
        "\n",
        "Create pytorch dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O1p1rMbac3X",
        "outputId": "7decf898-8284-44b0-a817-dba5c97ce940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /root/.genomic_benchmarks/human_enhancers_cohn.zip... Done.\n",
            "Unzipping...Done.\n"
          ]
        }
      ],
      "source": [
        "train_dset =  HumanEnhancersCohn('train', version=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JACj9LGQac3Y"
      },
      "source": [
        "# Print out information about the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "Vkg8YbBtac3Z",
        "outputId": "be834dc4-c4a3-4755-cdaf-7c1f13d6ea58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset `human_enhancers_cohn` has 2 classes: negative, positive.\n",
            "\n",
            "All lenghts of genomic intervals equals 500.\n",
            "\n",
            "Totally 27791 sequences have been found, 20843 for training and 6948 for testing.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>10422</td>\n",
              "      <td>3474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>10421</td>\n",
              "      <td>3474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          train  test\n",
              "negative  10422  3474\n",
              "positive  10421  3474"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "info(\"human_enhancers_cohn\", 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO9Gs2tFac3a"
      },
      "source": [
        "# Tokenizer and vocab\n",
        "\n",
        "Create tokenizer for the dataset, so we can numericalize the data and feed them to neural network. \n",
        "From the dataset info we can notice that all sequences are of the same length, hence we will use no padding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRY9_1NRac3b",
        "outputId": "444eb109-2951-4320-f6b7-6329141759de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab len: 7\n",
            "{'<eos>': 6, '<bos>': 1, '<unk>': 0, 'C': 5, 'A': 2, 'T': 3, 'G': 4}\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer(LetterTokenizer())\n",
        "vocabulary = build_vocab(train_dset, tokenizer, use_padding=False)\n",
        "\n",
        "print(\"vocab len:\" ,vocabulary.__len__())\n",
        "print(vocabulary.get_stoi())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8_bIg2ac3c"
      },
      "source": [
        "# Dataloader and batch preparation\n",
        "\n",
        "We will create pytorch data loader, which will preprocess and batch the data for the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVjAMp9Qac3d",
        "outputId": "18adf004-7381-4beb-91a6-c8c8ba9f1750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))\n",
        "\n",
        "collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
        "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfVIba63ac3d"
      },
      "source": [
        "# Model\n",
        "We will initialize our model.\n",
        "From the dataset info, we know that all inputs are 500 characters long, and the number of classes is 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KXHLGdMsac3e"
      },
      "outputs": [],
      "source": [
        "model = CNN(\n",
        "    number_of_classes=2,\n",
        "    vocab_size=vocabulary.__len__(),\n",
        "    embedding_dim=100,\n",
        "    input_len=500\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXSfvH6bac3e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FThlj7Y6ac3e",
        "outputId": "578ef82c-a861-4d68-ffe3-262f4c75b37e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train metrics: \n",
            " Accuracy: 65.4%, Avg loss: 0.645070 \n",
            "\n",
            "Epoch 1\n",
            "Train metrics: \n",
            " Accuracy: 67.7%, Avg loss: 0.637024 \n",
            "\n",
            "Epoch 2\n",
            "Train metrics: \n",
            " Accuracy: 68.1%, Avg loss: 0.637149 \n",
            "\n",
            "Epoch 3\n",
            "Train metrics: \n",
            " Accuracy: 67.3%, Avg loss: 0.636471 \n",
            "\n",
            "Epoch 4\n",
            "Train metrics: \n",
            " Accuracy: 70.7%, Avg loss: 0.630674 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.train(train_loader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzADCTJac3f"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "akWdmV6nac3f"
      },
      "outputs": [],
      "source": [
        "test_dset = HumanEnhancersCohn('test', version=0)\n",
        "test_loader = DataLoader(test_dset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "predictions = []\n",
        "for x,y in test_loader:\n",
        "  output = torch.round(model(x))\n",
        "  for prediction in output.tolist():\n",
        "    predictions.append(prediction[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyROI4K_NYtq",
        "outputId": "97b8f282-b1d3-4794-e99c-c027fde9d9a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.40943812595484635"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from genomic_benchmarks.data_check.info import labels_in_order\n",
        "\n",
        "labels = labels_in_order(dset_name='human_enhancers_cohn')\n",
        "f1_score(labels, predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "How_To_Train_CNN_Classifier_With_Pytorch.ipynb",
      "provenance": []
    },
    "environment": {
      "name": "pytorch-gpu.1-9.m75",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
    },
    "interpreter": {
      "hash": "af167c304fdc99884e31a029277e630a5b00036f91292350fffdeed1d15b16ff"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('genomic_benchmarks': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
