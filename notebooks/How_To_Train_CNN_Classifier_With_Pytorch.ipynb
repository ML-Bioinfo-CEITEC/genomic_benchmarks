{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Train CNN Classifier With Pytorch\n",
    "\n",
    "This notebook demonstrates how to use `genomic_benchmarks` to train a neural network classifier on one of its benchmark datasets [human_nontata_promoters](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/tree/main/docs/human_nontata_promoters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intall torchtext\n",
    "\n",
    "This demo uses utilities from torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn\n",
    "from genomic_benchmarks.models.torch_cnn import CNN\n",
    "from genomic_benchmarks.dataset_getters.utils import coll_factory, LetterTokenizer, build_vocab\n",
    "from genomic_benchmarks.data_check import info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the dataset\n",
    "\n",
    "Create pytorch dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /home/martinekvlastimil95/.genomic_benchmarks/translated_human_enhancers_cohn/human_enhancers_cohn.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "train_dset =  HumanEnhancersCohn('train', version=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `human_enhancers_cohn` has 2 classes: negative, positive.\n",
      "\n",
      "All lenghts of genomic intervals equals 500.\n",
      "\n",
      "Totally 27791 sequences have been found, 20843 for training and 6948 for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinekvlastimil95/repos/testproject/genomic_benchmarks/src/genomic_benchmarks/loc2seq/with_biopython.py:96: UserWarning: No version specified. Using version 0.\n",
      "  warnings.warn(f\"No version specified. Using version {metadata['version']}.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>10422</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>10421</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train  test\n",
       "negative  10422  3474\n",
       "positive  10421  3474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(\"human_enhancers_cohn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and vocab\n",
    "\n",
    "Create tokenizer for the dataset, so we can numericalize the data and feed them to neural network. \n",
    "From the dataset info we can notice that all sequences are of the same length, hence we will use no padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 6\n",
      "{'<eos>': 5, 'G': 4, 'A': 3, 'C': 2, 'T': 1, '<bos>': 0}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(LetterTokenizer())\n",
    "vocabulary = build_vocab(train_dset, tokenizer, use_padding=False)\n",
    "\n",
    "print(\"vocab len:\" ,vocabulary.__len__())\n",
    "print(vocabulary.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader and batch preparation\n",
    "\n",
    "We will create pytorch data loader, which will preprocess and batch the data for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "collate = coll_factory(vocabulary, tokenizer, device, pad_to_length = None)\n",
    "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We will initialize our model.\n",
    "From the dataset info, we know that all inputs are 500 characters long, and the number of classes is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    number_of_classes=2,\n",
    "    vocab_size=vocabulary.__len__(),\n",
    "    embedding_dim=100,\n",
    "    input_len=500\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train metrics: \n",
      " Accuracy: 65.0%, Avg loss: 0.653522 \n",
      "\n",
      "Epoch 1\n",
      "Train metrics: \n",
      " Accuracy: 66.2%, Avg loss: 0.650962 \n",
      "\n",
      "Epoch 2\n",
      "Train metrics: \n",
      " Accuracy: 67.3%, Avg loss: 0.643297 \n",
      "\n",
      "Epoch 3\n",
      "Train metrics: \n",
      " Accuracy: 68.4%, Avg loss: 0.641080 \n",
      "\n",
      "Epoch 4\n",
      "Train metrics: \n",
      " Accuracy: 68.7%, Avg loss: 0.636772 \n",
      "\n",
      "Epoch 5\n",
      "Train metrics: \n",
      " Accuracy: 69.9%, Avg loss: 0.631040 \n",
      "\n",
      "Epoch 6\n",
      "Train metrics: \n",
      " Accuracy: 70.1%, Avg loss: 0.630158 \n",
      "\n",
      "Epoch 7\n",
      "Train metrics: \n",
      " Accuracy: 67.5%, Avg loss: 0.633789 \n",
      "\n",
      "Epoch 8\n",
      "Train metrics: \n",
      " Accuracy: 71.4%, Avg loss: 0.625305 \n",
      "\n",
      "Epoch 9\n",
      "Train metrics: \n",
      " Accuracy: 73.0%, Avg loss: 0.625349 \n",
      "\n",
      "Epoch 10\n",
      "Train metrics: \n",
      " Accuracy: 71.5%, Avg loss: 0.625071 \n",
      "\n",
      "Epoch 11\n",
      "Train metrics: \n",
      " Accuracy: 71.2%, Avg loss: 0.622787 \n",
      "\n",
      "Epoch 12\n",
      "Train metrics: \n",
      " Accuracy: 73.3%, Avg loss: 0.621158 \n",
      "\n",
      "Epoch 13\n",
      "Train metrics: \n",
      " Accuracy: 73.0%, Avg loss: 0.620399 \n",
      "\n",
      "Epoch 14\n",
      "Train metrics: \n",
      " Accuracy: 72.4%, Avg loss: 0.622846 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 176563cDPQ5Y094WyoSBF02QjoVQhWuCh into /home/martinekvlastimil95/.genomic_benchmarks/translated_human_enhancers_cohn/human_enhancers_cohn.zip... Done.\n",
      "Unzipping...Done.\n",
      "test_loss  140.27838283777237\n",
      "num_batches 218\n",
      "correct 4736\n",
      "size 6948\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.643479 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dset = HumanEnhancersCohn('test', version=0)\n",
    "test_loader = DataLoader(test_dset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "model.test(test_loader)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "c0ce2704b40e8f5ab266e985c6dfdf52dff618cf568b21da2fc16697057ae39e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('benchenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
